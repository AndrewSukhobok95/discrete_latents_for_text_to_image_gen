{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ede8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms as torch_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27679379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")\n",
    "\n",
    "from modules.dvae.model import DVAE\n",
    "from modules.common_blocks import ResidualStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e016738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the accuracy on the test dataset\n",
    "def compute_accuracy(dvae, clf, testloader, device):\n",
    "    clf.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            latent = dvae.q_encode(images, hard=True)\n",
    "            outputs = clf(latent)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95d873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 embedding_dim,\n",
    "                 num_blocks):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.resid = ResidualStack(\n",
    "            in_channels=embedding_dim, \n",
    "            out_channels=embedding_dim, \n",
    "            num_residual_layers=num_blocks, \n",
    "            bias=True, \n",
    "            use_bn=True, \n",
    "            final_relu=False)\n",
    "        \n",
    "        channels_dims = [\n",
    "            embedding_dim // 2,\n",
    "            embedding_dim // 4,\n",
    "        ]\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=embedding_dim,\n",
    "                      out_channels=channels_dims[0],\n",
    "                      kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(num_features=channels_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=channels_dims[0], \n",
    "                      out_channels=channels_dims[1], \n",
    "                      kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(num_features=channels_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=channels_dims[1], \n",
    "                      out_channels=n_classes, \n",
    "                      kernel_size=3, padding=0),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=n_classes, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resid(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x.squeeze())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4df45a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrEncoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_attn_heads, n_hidden=64, dropout_prob=0.1):\n",
    "        super(TrEncoderBlock, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_attn_heads)\n",
    "        self.ln1 = nn.LayerNorm(n_features)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_hidden, n_features)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(n_features)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, pad_mask=None, attn_mask=None):\n",
    "        xn = self.ln1(x)\n",
    "        dx, _ = self.attn(query=xn, key=xn, value=xn, \n",
    "                          key_padding_mask=pad_mask, \n",
    "                          attn_mask=attn_mask)\n",
    "        x = x + self.dropout1(dx)\n",
    "        \n",
    "        xn = self.ln2(x)\n",
    "        dx = self.mlp(xn)\n",
    "        x = x + self.dropout2(dx)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 embedding_dim,\n",
    "                 hidden_height,\n",
    "                 hidden_width,\n",
    "                 num_blocks,\n",
    "                 n_attn_heads,\n",
    "                 hidden_dim,\n",
    "                 dropout_prob):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "\n",
    "        \n",
    "        num_latent_positions = hidden_height * hidden_width + 1\n",
    "        self.pe = nn.Parameter(torch.randn(1, num_latent_positions, embedding_dim))\n",
    "        \n",
    "        self.lin_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        self.tr_encoder_blocks = nn.ModuleList([\n",
    "            TrEncoderBlock(n_features=embedding_dim,\n",
    "                           n_attn_heads=n_attn_heads,\n",
    "                           n_hidden=hidden_dim,\n",
    "                           dropout_prob=dropout_prob)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, img_latent):\n",
    "        b, c, h, w = img_latent.size()\n",
    "        x = img_latent.view(b, c, h * w).permute(0, 2, 1)  # -> b, h*w, c\n",
    "\n",
    "        x = self.lin_proj(x)\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pe\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # -> h*w, b, c\n",
    "\n",
    "        for i, block in enumerate(self.tr_encoder_blocks):\n",
    "            x = block(x)\n",
    "\n",
    "        #cls_input = x.mean(dim=0)\n",
    "        cls_input = x[0, :, :]\n",
    "            \n",
    "        cls = self.mlp_head(cls_input).squeeze()\n",
    "\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "200583b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DEVICE                      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    img_channels                = 1\n",
    "    vocab_size                  = 128\n",
    "\n",
    "    noise_dim                   = 100\n",
    "    hidden_height               = 7\n",
    "    hidden_width                = 7\n",
    "\n",
    "    num_blocks                  = 8\n",
    "    n_attn_heads                = 8\n",
    "    hidden_dim                  = 256\n",
    "    dropout_prob                = 0.1\n",
    "\n",
    "    dvae_num_x2upsamples        = 2\n",
    "    dvae_num_resids_downsample  = 3\n",
    "    dvae_num_resids_bottleneck  = 4\n",
    "    dvae_hidden_dim             = 256\n",
    "\n",
    "    load_dvae_path              = \"/m/home/home8/82/sukhoba1/data/Desktop/TA-VQVAE/models/dvae_M_mnist/\"\n",
    "    dvae_model_name             = \"dvae_M_mnist\"\n",
    "    data_path                   = \"/m/home/home8/82/sukhoba1/data/Desktop/TA-VQVAE/data/MNIST/\"\n",
    "\n",
    "    NUM_EPOCHS                  = 10\n",
    "    BATCH_SIZE                  = 512\n",
    "    LR                          = 0.001\n",
    "    LR_gamma                    = 0.1\n",
    "    step_LR_milestones          = [90]\n",
    "\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ed9555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = torch_transforms.Compose([\n",
    "    torch_transforms.RandomRotation(10),\n",
    "    torch_transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    CONFIG.data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    CONFIG.data_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=data_transforms)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6a2451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvae = DVAE(\n",
    "    in_channels=CONFIG.img_channels,\n",
    "    vocab_size=CONFIG.vocab_size,\n",
    "    num_x2downsamples=CONFIG.dvae_num_x2upsamples,\n",
    "    num_resids_downsample=CONFIG.dvae_num_resids_downsample,\n",
    "    num_resids_bottleneck=CONFIG.dvae_num_resids_bottleneck,\n",
    "    hidden_dim=CONFIG.dvae_hidden_dim)\n",
    "\n",
    "dvae.eval()\n",
    "dvae.load_model(\n",
    "    root_path=CONFIG.load_dvae_path,\n",
    "    model_name=CONFIG.dvae_model_name)\n",
    "dvae.to(CONFIG.DEVICE)\n",
    "\n",
    "\n",
    "# clf = CNNClassifier(\n",
    "#     n_classes=10,\n",
    "#     embedding_dim=CONFIG.vocab_size,\n",
    "#     num_blocks=CONFIG.num_blocks)\n",
    "\n",
    "clf = ViTClassifier(\n",
    "    n_classes=10,\n",
    "    embedding_dim=CONFIG.vocab_size,\n",
    "    hidden_height=CONFIG.hidden_height,\n",
    "    hidden_width=CONFIG.hidden_width,\n",
    "    num_blocks=CONFIG.num_blocks,\n",
    "    n_attn_heads=CONFIG.n_attn_heads,\n",
    "    hidden_dim=CONFIG.hidden_dim,\n",
    "    dropout_prob=CONFIG.dropout_prob)\n",
    "\n",
    "clf.train()\n",
    "clf.to(CONFIG.DEVICE)\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d30ed21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda\n",
      "Epoch: 0 Iter: 55 Loss: 2.302194833755493 Test Accuracy: 0.1016\n",
      "Epoch: 0 Iter: 110 Loss: 1.7957141399383545 Test Accuracy: 0.3501\n",
      "Epoch: 1 Iter: 165 Loss: 0.8636090755462646 Test Accuracy: 0.6912\n",
      "Epoch: 1 Iter: 220 Loss: 0.46149003505706787 Test Accuracy: 0.862\n",
      "Epoch: 2 Iter: 275 Loss: 0.2852141261100769 Test Accuracy: 0.9076\n",
      "Epoch: 2 Iter: 330 Loss: 0.21170789003372192 Test Accuracy: 0.9173\n",
      "Epoch: 3 Iter: 385 Loss: 0.24467302858829498 Test Accuracy: 0.9371\n",
      "Epoch: 3 Iter: 440 Loss: 0.14906670153141022 Test Accuracy: 0.9388\n",
      "Epoch: 4 Iter: 495 Loss: 0.2537703514099121 Test Accuracy: 0.9465\n",
      "Epoch: 4 Iter: 550 Loss: 0.14439043402671814 Test Accuracy: 0.9453\n",
      "Epoch: 5 Iter: 605 Loss: 0.1485423892736435 Test Accuracy: 0.9463\n",
      "Epoch: 5 Iter: 660 Loss: 0.1970188170671463 Test Accuracy: 0.9526\n",
      "Epoch: 6 Iter: 715 Loss: 0.0956149473786354 Test Accuracy: 0.9523\n",
      "Epoch: 6 Iter: 770 Loss: 0.12703555822372437 Test Accuracy: 0.9584\n",
      "Epoch: 6 Iter: 825 Loss: 0.1273706555366516 Test Accuracy: 0.9637\n",
      "Epoch: 7 Iter: 880 Loss: 0.16273027658462524 Test Accuracy: 0.9604\n",
      "Epoch: 7 Iter: 935 Loss: 0.134141206741333 Test Accuracy: 0.9604\n",
      "Epoch: 8 Iter: 990 Loss: 0.08483684808015823 Test Accuracy: 0.9587\n",
      "Epoch: 8 Iter: 1045 Loss: 0.10518421977758408 Test Accuracy: 0.9668\n",
      "Epoch: 9 Iter: 1100 Loss: 0.110568568110466 Test Accuracy: 0.9674\n",
      "Epoch: 9 Iter: 1155 Loss: 0.09157467633485794 Test Accuracy: 0.9632\n"
     ]
    }
   ],
   "source": [
    "print(\"Device in use: {}\".format(CONFIG.DEVICE))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=CONFIG.LR)\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(CONFIG.NUM_EPOCHS):\n",
    "    for x, label in train_loader:\n",
    "        \n",
    "        label = label.to(CONFIG.DEVICE)\n",
    "        x = x.to(CONFIG.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latent = dvae.q_encode(x, hard=True)\n",
    "            #latent = dvae.sm_encode(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = clf(latent)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration % 55 == 0:\n",
    "            acc = compute_accuracy(dvae, clf, test_loader, device=CONFIG.DEVICE)\n",
    "            print(\"Epoch: {} Iter: {} Loss: {} Test Accuracy: {}\".format(\n",
    "                epoch, iteration, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f277aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7aa9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bac4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eefe19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da390e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2cfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad3ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba3e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3be78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da711ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf2f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a83450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
